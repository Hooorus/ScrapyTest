### `settings.py`内的设置
1. 添加爬取时候的用户代理，此代码是为了模拟浏览器代理<br>
`USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36"`
2. 我不想遵从机器人协议进行数据采集<br>
`ROBOTSTXT_OBEY = False`
3. 日志等级为ERROR才显示<br>
`LOG_LEVEL = 'ERROR'`

### 执行脚本
1. 运行爬虫
`scrapy crawl <spider_name> -o <result.json>`<br>
<spider_name>: 在spiders文件夹内的某个爬虫里面的name属性<br>
-o 持久化存储<br>
<result.json>: 结果存储在这
2. 翻页操作
